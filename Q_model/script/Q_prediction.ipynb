{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.path.append('.')\n",
    "import arg_utils as qm9_utils\n",
    "import os\n",
    "from mgp import layers\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "from torch_sparse import coalesce\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks = ['alpha', 'gap', 'homo', 'lumo', 'mu', 'Cv', 'G', 'H', 'r2', 'U', 'U0', 'zpve']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='QM9 Example')\n",
    "parser.add_argument('--config_path', type=str, default='Q_model/config/finetune_qm9.yml', metavar='N',\n",
    "                    help='Path of config yaml.')\n",
    "parser.add_argument('--property', type=str, default='', metavar='N',\n",
    "                    help='Property to predict.')\n",
    "parser.add_argument('--model_name', type=str, default='', metavar='N',\n",
    "                    help='Model name.')\n",
    "parser.add_argument('--restore_path', type=str, default='', metavar='N',\n",
    "                    help='Restore path.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.float32\n",
    "\n",
    "with open(args.config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "config = EasyDict(config)\n",
    "\n",
    "if args.property != '':\n",
    "    config.train.property = args.property\n",
    "\n",
    "if args.model_name != '':\n",
    "    config.model.name = args.model_name\n",
    "\n",
    "if args.restore_path != '':\n",
    "    config.train.restore_path = args.restore_path\n",
    "\n",
    "os.makedirs(config.train.save_path + \"/\" + config.model.name + \"/\" + config.train.property, exist_ok=True)\n",
    "\n",
    "# fix seed\n",
    "seed = config.train.seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "print('fix seed:', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, model_path):\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    new_dict = OrderedDict()\n",
    "    for k, v in state['model'].items():\n",
    "        if k.startswith('module.model.'):\n",
    "            new_dict[k[13:]] = v\n",
    "        if k.startswith('model.'):\n",
    "            new_dict[k[6:]] = v\n",
    "        # if k.startswith('module.node_dec.'):\n",
    "        #     new_dict[k[7:]] = v\n",
    "    model.load_state_dict(new_dict, strict=False)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders, charge_scale = load_dataset.retrieve_dataloaders(config.data.base_path, config.train.batch_size, config.train.num_workers)\n",
    "# compute mean and mean absolute deviation\n",
    "meann, mad = qm9_utils.compute_mean_mad(dataloaders, config.train.property)\n",
    "\n",
    "model = layers.EGNN_finetune_last(in_node_nf=config.model.max_atom_type * (config.model.charge_power + 1),\n",
    "                                  in_edge_nf=0 if config.model.no_edge_types else 4, hidden_nf=config.model.hidden_dim,\n",
    "                                  n_layers=config.model.n_layers,\n",
    "                                  attention=config.model.attention, use_layer_norm=config.model.layernorm).to(device)\n",
    "\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "if config.train.restore_path:\n",
    "    encoder_param = load_model(model, config.train.restore_path)\n",
    "    print('load model from', config.train.restore_path)\n",
    "\n",
    "optimizer = optim.Adam([param for name, param in model.named_parameters()], lr=config.train.lr,\n",
    "                       weight_decay=float(config.train.weight_decay))\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config.train.epochs,\n",
    "                                                          eta_min=float(config.train.min_lr))\n",
    "loss_l1 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_input(atom_type, max_atom_type=100, charge_power=2):\n",
    "    one_hot = nn.functional.one_hot(atom_type, max_atom_type)\n",
    "    charge_tensor = (atom_type.unsqueeze(-1) / max_atom_type).pow(\n",
    "        torch.arange(charge_power + 1., dtype=torch.float32).to(atom_type))\n",
    "    charge_tensor = charge_tensor.view(atom_type.shape + (1, charge_power + 1))\n",
    "    atom_scalars = (one_hot.unsqueeze(-1) * charge_tensor).view(atom_type.shape + (-1,))\n",
    "    return atom_scalars\n",
    "\n",
    "\n",
    "def binarize(x):\n",
    "    return torch.where(x > 0.5, torch.ones_like(x), torch.zeros_like(x))\n",
    "\n",
    "\n",
    "def get_higher_order_adj_matrix(adj, order):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        adj:        (N, N)\n",
    "        type_mat:   (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    adj_mats = [torch.eye(adj.size(0), device=adj.device), \\\n",
    "                binarize(adj + torch.eye(adj.size(0), device=adj.device))]\n",
    "    for i in range(2, order + 1):\n",
    "        adj_mats.append(binarize(adj_mats[i - 1] @ adj_mats[1]))\n",
    "    order_mat = torch.zeros_like(adj).float()\n",
    "\n",
    "    for i in range(1, order + 1):\n",
    "        order_mat += (adj_mats[i] - adj_mats[i - 1]) * i\n",
    "    return order_mat.long()\n",
    "\n",
    "\n",
    "def add_high_order_edges(adj_mat):\n",
    "    adj_order = get_higher_order_adj_matrix(adj_mat, 3)\n",
    "    type_highorder = torch.where(adj_order > 1, adj_order, torch.zeros_like(adj_order))\n",
    "    type_mat = adj_mat\n",
    "    assert (type_mat * type_highorder == 0).all()\n",
    "    type_new = type_mat + type_highorder\n",
    "    edge_index, edge_type = dense_to_sparse(type_new)\n",
    "    return edge_index, edge_type\n",
    "\n",
    "\n",
    "def gen_adj_matrix(pos, mask):\n",
    "    # pos : batch * n_nodes * 3\n",
    "    batch, nodes = mask.shape\n",
    "    batch_adj = torch.norm(pos.unsqueeze(1) - pos.unsqueeze(2), p=2, dim=-1)  # batch * n * n\n",
    "    batch_mask = mask[:, :, None] * mask[:, None, :]  # batch * n * n\n",
    "    batch_mask = batch_mask.bool() & (batch_adj <= 1.6) & (~torch.eye(nodes).to(mask).bool())  # batch * n * n\n",
    "    return torch.block_diag(*batch_mask)\n",
    "\n",
    "\n",
    "def gen_fully_connected(pos, mask):\n",
    "    batch, nodes = mask.shape\n",
    "    batch_mask = mask[:, :, None] * mask[:, None, :]\n",
    "    batch_mask = batch_mask.bool() & (~torch.eye(nodes).to(mask).bool())\n",
    "    batch_mask = torch.block_diag(*batch_mask)\n",
    "    edge_index, edge_type = dense_to_sparse(batch_mask)\n",
    "    return edge_index, edge_type\n",
    "\n",
    "def gen_fully_connected_with_hop(pos, mask):\n",
    "    batch, nodes = mask.shape\n",
    "    batch_adj = torch.norm(pos.unsqueeze(1) - pos.unsqueeze(2), p=2, dim=-1)  # batch * n * n\n",
    "    batch_mask_fc = mask[:, :, None] * mask[:, None, :]  # batch * n * n\n",
    "    # 1.6 is an empirically reasonable cutoff to distinguish the existence of bonds for stable small molecules\n",
    "    batch_mask = batch_mask_fc.bool() & (batch_adj <= 1.6) & (~torch.eye(nodes).to(mask).bool())  # batch * n * n\n",
    "    batch_mask = torch.block_diag(*batch_mask)\n",
    "    adj_order = get_higher_order_adj_matrix(batch_mask, 3)\n",
    "    type_highorder = torch.where(adj_order > 1, adj_order, torch.zeros_like(adj_order))\n",
    "    fc_mask = batch_mask_fc.bool() & (~torch.eye(nodes).to(mask).bool())\n",
    "    fc_mask = torch.block_diag(*fc_mask)\n",
    "    type_new = batch_mask + type_highorder + fc_mask\n",
    "    edge_index, edge_type = dense_to_sparse(type_new)\n",
    "    return edge_index, edge_type - 1\n",
    "\n",
    "\n",
    "def train(epoch, loader, config, partition='train'):\n",
    "    res = {'loss': 0, 'counter': 0, 'loss_arr': []}\n",
    "    train_npred = []\n",
    "    train_ntrue = []\n",
    "    valid_npred = []\n",
    "    valid_ntrue = []\n",
    "    test_npred = []\n",
    "    test_ntrue = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        if partition == 'train':\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        batch_size, n_nodes, _ = data['positions'].size()\n",
    "        atom_positions = data['positions'].view(batch_size, n_nodes, -1).to(device, dtype)\n",
    "        atom_mask = data['atom_mask'].view(batch_size, n_nodes).to(device)\n",
    "        edge_mask = data['edge_mask'].to(device)\n",
    "        charges = data['charges'].to(device, torch.long)\n",
    "        charges = charges.view(batch_size * n_nodes, -1)\n",
    "        nodes = process_input(charges, max_atom_type=config.model.max_atom_type, charge_power=config.model.charge_power)\n",
    "        nodes = nodes.view(batch_size * n_nodes, -1)\n",
    "        if config.model.no_edge_types:\n",
    "            edges, edge_types = gen_fully_connected(atom_positions, atom_mask)\n",
    "            edge_attr = None\n",
    "        else:\n",
    "            edges, edge_types = gen_fully_connected_with_hop(atom_positions, atom_mask)\n",
    "            edge_attr = nn.functional.one_hot(edge_types, 4)\n",
    "        label = data[config.train.property].to(device, dtype)\n",
    "        atom_positions = atom_positions.view(batch_size * n_nodes, -1)\n",
    "        atom_mask = atom_mask.view(batch_size * n_nodes, -1)\n",
    "        pred = model(h=nodes, x=atom_positions, edges=edges, edge_attr=edge_attr, node_mask=atom_mask, n_nodes=n_nodes,\n",
    "                     adapter=config.train.property)\n",
    "\n",
    "        if partition == 'train':\n",
    "            loss = loss_l1(pred, (label - meann) / mad)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_npred.append(pred.detach().cpu().numpy())\n",
    "            train_ntrue.append(label.cpu().numpy())\n",
    "\n",
    "\n",
    "        elif partition == 'valid':\n",
    "            loss = loss_l1(pred, (label - meann) / mad)\n",
    "            valid_npred.append(pred.detach().cpu().numpy())\n",
    "            valid_ntrue.append(label.cpu().numpy())\n",
    "\n",
    "        else:\n",
    "            loss = loss_l1(pred, (label - meann) / mad)\n",
    "            test_npred.append(pred.detach().cpu().numpy())\n",
    "            test_ntrue.append(label.cpu().numpy())\n",
    "\n",
    "        res['loss'] += loss.item() * batch_size\n",
    "        res['counter'] += batch_size\n",
    "        res['loss_arr'].append(loss.item())\n",
    "\n",
    "        predict = mad * pred + meann\n",
    "        predict = predict.detach().cpu().numpy()\n",
    "        label = label.detach().cpu().numpy()\n",
    "        predictions.extend(predict)\n",
    "        labels.extend(label)\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    predictions = predictions.reshape(-1)\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    # labels = np.array(labels)\n",
    "    # predictions.detach().cpu().nnumpy()\n",
    "    # labels.detach().cpu().numpy()\n",
    "    xdata = pd.DataFrame({'predictions': predictions, 'labels': labels})\n",
    "    xdata.to_csv('Q_model/sdf/778Q/pred.csv', index=None)\n",
    "\n",
    "    if partition == 'train':\n",
    "        # print(train_ntrue)\n",
    "        # print(train_npred)\n",
    "        train_true = np.concatenate(np.array(train_ntrue), 0)\n",
    "        train_pred = np.concatenate(np.array(train_npred), 0)\n",
    "        # print(train_true)\n",
    "        # print(train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(mad * train_pred + meann, train_true))\n",
    "        train_mae = mean_absolute_error(mad * train_pred + meann, train_true)\n",
    "        r2_train = r2_score(train_true, mad * train_pred + meann)\n",
    "        rmse = train_rmse\n",
    "        r2 = r2_train\n",
    "        mae = train_mae\n",
    "\n",
    "    elif partition == 'valid':\n",
    "        valid_true = np.concatenate(np.array(valid_ntrue), 0)\n",
    "        valid_pred = np.concatenate(np.array(valid_npred), 0)\n",
    "        valid_rmse = np.sqrt(mean_squared_error(mad * valid_pred + meann, valid_true))\n",
    "        valid_mae = mean_absolute_error(mad * valid_pred + meann, valid_true)\n",
    "        r2_valid = r2_score(valid_true, mad * valid_pred + meann)\n",
    "        rmse = valid_rmse\n",
    "        r2 = r2_valid\n",
    "        mae = valid_mae\n",
    "\n",
    "    else:\n",
    "        test_true = np.concatenate(np.array(test_ntrue), 0)\n",
    "        test_pred = np.concatenate(np.array(test_npred), 0)\n",
    "        test_rmse = np.sqrt(mean_squared_error(mad * test_pred + meann, test_true))\n",
    "        test_mae = mean_absolute_error(mad * test_pred + meann, test_true)\n",
    "        r2_test = r2_score(test_true, mad * test_pred + meann)\n",
    "        rmse = test_rmse\n",
    "        r2 = r2_test\n",
    "        mae = test_mae\n",
    "\n",
    "    return res['loss'] / res['counter'], rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'epochs': [], 'losess': [], 'best_val': 0, 'best_test': 0, 'best_epoch': 0}\n",
    "\n",
    "train_param = {'rmse': [], 'r2': [], 'mae': []}\n",
    "valid_param = {'rmse': [], 'r2': [], 'mae': []}\n",
    "test_param = {'rmse': [], 'r2': [], 'mae': []}\n",
    "\n",
    "all_train_loss, all_val_loss, all_test_loss = [], [], []\n",
    "path = config.train.save_path + \"/\" + config.model.name + \"/\" + config.train.property\n",
    "for epoch in range(0, config.train.epochs):\n",
    "    train_loss, train_rmse, train_mae, train_r2= train(epoch, dataloaders['train'], config, partition='train')\n",
    "\n",
    "    print('train_rmse:', train_rmse, 'train_mae:', train_mae, 'r2_train:', train_r2, 'epoch:', epoch)\n",
    "\n",
    "    train_param['rmse'].append(train_rmse)\n",
    "    train_param['r2'].append(train_r2)\n",
    "    train_param['mae'].append(train_mae)\n",
    "\n",
    "    all_train_loss.append(train_loss)\n",
    "    lr_scheduler.step()\n",
    "    if epoch % config.test.test_interval == 0:\n",
    "        val_loss, val_rmse, val_mae, val_r2 = train(epoch, dataloaders['valid'], config, partition='valid')\n",
    "\n",
    "        print('val_rmse:', val_rmse, 'val_mae:', val_mae, 'r2_val:', val_r2)\n",
    "\n",
    "        valid_param['rmse'].append(val_rmse)\n",
    "        valid_param['r2'].append(val_r2)\n",
    "        valid_param['mae'].append(val_mae)\n",
    "\n",
    "        test_loss, test_rmse, test_mae, test_r2 = train(epoch, dataloaders['test'], config, partition='test')\n",
    "\n",
    "        print('test_rmse:', test_rmse, 'test_mae:', test_mae, 'r2_test:', test_r2)\n",
    "\n",
    "        test_param['rmse'].append(test_rmse)\n",
    "        test_param['r2'].append(test_r2)\n",
    "        test_param['mae'].append(test_mae)\n",
    "\n",
    "        res['epochs'].append(epoch)\n",
    "        # res['losess'].append(test_loss)\n",
    "        print('res_best:', res['best_test'])\n",
    "        if test_r2 > res['best_test']:\n",
    "            res['best_test'] = test_r2\n",
    "            # res['best_test'] = test_loss\n",
    "            res['best_epoch'] = epoch\n",
    "            state = {\n",
    "                 \"model\": model.state_dict(),\n",
    "                 \"optimizer\": optimizer.state_dict(),\n",
    "                 \"scheduler\": lr_scheduler.state_dict(),\n",
    "                 \"epoch\": epoch\n",
    "                }\n",
    "            torch.save(state, path + \"/checkpoint.pth\")\n",
    "            all_val_loss.append(val_loss)\n",
    "            all_test_loss.append(test_loss)\n",
    "    # save current loss\n",
    "        xdata = pd.DataFrame(\n",
    "            {'train_rmse': train_param['rmse'], 'r2_train': train_param['r2'], 'train_mae': train_param['mae'],\n",
    "             'val_rmse': valid_param['rmse'],  'r2_val': valid_param['r2'], 'val_mae': valid_param['mae'],\n",
    "             'test_rmse:': test_param['rmse'], 'test_mae:': test_param['mae'], 'r2_test:': test_param['r2']})\n",
    "        xdata.to_csv('Q_model/sdf/778Q/results.csv', index=None)\n",
    "\n",
    "    loss_file = path + '/loss.pkl'\n",
    "    with open(loss_file, 'wb') as f:\n",
    "        pkl.dump((all_train_loss, all_val_loss, all_test_loss), f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
